---
title: "Bayesian Multilevel Regression - Are Bayern Munich Good or is the Bundesliga Bad?"
author: "Paul Johnson"
format: gfm
editor_options: 
  chunk_output_type: console
---

```{r setup}
#| include: false

# packages
suppressPackageStartupMessages({
  library(dplyr)
  library(ggplot2)
  library(patchwork)
  library(rstanarm)
})

# plot theme
# font to (approximately) match GitHub markdown output and FTW article body
theme_set(theme_minimal(base_family = "Inter"))
# font to match my personal website
# theme_set(theme_minimal(base_family = "Jet Brains Mono"))
bayesplot::color_scheme_set("teal")

# data
club_resources <- readr::read_rds(here::here("data", "club_resources.rds"))

```

Data often contain hierarchical or group structures that leads to a structural dependence within-group. That makes modelling the data in a flat structure difficult, because there will be correlated errors within each group.

The classic example of a hierarchical data structure is student test results, nested within different schools or within different classes. You would expect there to be a correlation between results within a certain school or class, as teaching methods, resources, and other group-level factors effect students learning and subsequent test results. In order to get a more accurate picture of 
 
This can be resolved using the model of many names, such as multilevel models, mixed effects models, or hierarchical models.[^1] 

[^1]: I don't like the names like mixed or fixed/random effects, because I don't think they are particularly informative. Referring to either multilevel or hierarchical models is much easier for someone that is unfamiliar with the idea to intuit what it means. I'm not sure that there is any consensus in statistics as to which of multilevel or hierarchical is better, but I will refer to multilevel models, just because I had to pick one for consistency, and I tend to think multilevel regression is a slightly more intuitive term.

Multilevel models...


# Preparing Data for Modelling

-   Removing the Austrian Bundesliga due to issues with data quality
-   Some basic transformations and type changes to make sure the data is ready for modelling

```{r data-prep}

club_resources <-
  club_resources %>%
  filter(league != "Austrian Bundesliga") %>%
  mutate(
    weighted_injuries = days_injured/num_players,
    log_value = log(value)
  )

model_df <- 
  club_resources %>%
  mutate(
    league = forcats::as_factor(league),
    season = forcats::as_factor(season),
    squad = forcats::as_factor(squad),
    log_value = (log_value - mean(log_value))/sd(log_value),
    value = (value - mean(value))/sd(value),
    pts = (pts - mean(pts))/sd(pts),
    net_spend = (net_spend - mean(net_spend))/sd(net_spend),
    num_players = (num_players - mean(num_players))/sd(num_players),
    days_injured = (days_injured - mean(days_injured))/sd(days_injured),
    weighted_injuries = (weighted_injuries - mean(weighted_injuries))/sd(weighted_injuries)
    )

```

# Multilevel Regression

## Varying Intercepts Model with No Predictors

-   So the log(value) models perform slightly worse than the non-transformed models, in terms of $Loo R^2$. Is this because there is less variance to explain?
-   I need to give this some thought and explain what is driving this issue.

```{r no-predictors}

lmer1 <-
  stan_lmer(pts ~ 1 + (1 | league),
      data = model_df)  

print(lmer1)

median(loo_R2(lmer1))

as_tibble(loo_R2(lmer1)) %>%
  ggplot(aes(value)) +
  geom_density() +
  labs(x = expression("LOO R"^2), y = NULL)

```

## Varying Intercepts Model with Squad Value Predictor

```{r varying-intercepts}

lmer2 <- 
  stan_lmer(pts ~ 1 + value + (1|league), data = model_df)

print(lmer2)

bayesplot::mcmc_trace(lmer2)

bayesplot::ppc_dens_overlay(
  y = lmer2$y,
  yrep = posterior_predict(
    lmer2,
    draws = 100
    )
)
 
loo2 <- loo(lmer2)
```

## Varying Intercepts & Slopes Model

```{r varying-intercepts-and-slopes}

lmer3 <- 
  stan_lmer(pts ~ 1 + value + (1 + value|league), data = model_df)

bayesplot::mcmc_trace(lmer3)

bayesplot::ppc_dens_overlay(
  y = lmer3$y,
  yrep = posterior_predict(
    lmer3,
    draws = 100
    )
)

loo3 <- loo(lmer3, k_threshold = 0.7)

loo_compare(loo2, loo3)
```

## Varying Intercepts & Slopes Model with Multiple Predictors

-   Testing full varying intercepts and slopes model with multiple predictors, and compare against a varying intercepts equivalent.

```{r multivariable-varying-intercepts-and-slopes}

lmer4 <- 
  stan_lmer(
    pts ~ 1 + value + (1 + value |league),
    data = model_df
    )

print(lmer4)

bayesplot::mcmc_trace(lmer4)

p1 <- bayesplot::ppc_dens_overlay(
  y = lmer4$y,
  yrep = posterior_predict(
    lmer4,
    draws = 100
    )
)

prior_summary(lmer4)

```


```{r model-comp}

lmer5 <- 
  stan_lmer(
    pts ~ 1 + value + (1|squad) + (1|league),
    data = model_df
    )

print(lmer5)

bayesplot::mcmc_trace(lmer5)

p2 <- bayesplot::ppc_dens_overlay(
  y = lmer4$y,
  yrep = posterior_predict(
    lmer5,
    draws = 100
    )
)

p1+p2


loo4 <- loo(lmer4)
loo5 <- loo(lmer5, k_threshold = 0.7)

loo_compare(loo4, loo5)

```

# Simulating Bundesliga Points Total Based on Club Resources

-   Simulations will take the league and value of each team in the Bundesliga and simulate what the model would predict as their total points given this information.
-   Is there a time component here?
-   How do I present this information?

```{r sims}

df <- 
  club_resources %>%
  filter(league == "Bundesliga") %>%
  select(league, season, squad, value)

preds <- posterior_predict(lmer4, new_df = df)

pred_pts <- as_tibble( (preds*sd(club_resources$pts)) + mean(club_resources$pts) )

```
